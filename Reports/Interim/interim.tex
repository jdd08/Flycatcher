\documentclass[a4paper,11pt,titlepage]{report}
\usepackage{graphicx}
\usepackage{amssymb}
%\usepackage[margin=2.3cm]{geometry}
\usepackage{tikz}

\usetikzlibrary{calendar,snakes,calc,trees,positioning,arrows,chains,shapes.geometric,decorations.pathreplacing,decorations.pathmorphing,shapes,matrix,shapes.symbols}
\tikzset{
>=stealth',
  value/.style={
    text width=10em, 
    minimum height=3em, 
    text centered, 
    on chain},
  component/.style={
    rectangle, 
    rounded corners,
    draw=black, very thick,
    text width=10em, 
    minimum height=3em, 
    text centered, 
    on chain},
  line/.style={draw, thick, <-},
  element/.style={
    tape,
    top color=white,
    bottom color=blue!50!black!60!,
    minimum width=8em,
    draw=blue!40!black!90, very thick,
    text width=10em, 
    minimum height=3.5em, 
    text centered, 
    on chain},
  every join/.style={->, thick,shorten >=1pt},
  decoration={brace},
  category/.style={decorate},
  catlabel/.style={midway, right=2pt},
}

\begin{document}
	\title
	{
		{\Huge \textsf{Flycatcher} \\[0.2cm]}
		{\large \textsf{Automatic unit test generation for JavaScript}\\[0.5cm]}
		\begin{figure}[h]
			\centering
			\includegraphics[scale=0.3]{flycatcher.jpg}
		\end{figure}
	}
	\author
	{	
		{\emph{Interim Report}}\\[7cm]
		%{\large Jerome \textsc{de Lafargue}}\\
		%\texttt{\scriptsize jdd08@ic.ac.uk}\\[2cm]
		%{MEng Computing Individual Project}\\
		%{\emph{Interim Report}}\\[2.2cm]
		\textbf{\small Author:}
		{\small Jerome \textsc{de Lafargue}}\\
		\textbf{\small Supervisor:}
		{\small Susan \textsc{Eisenbach}}\\
		\textbf{\small Technical Supervisor:}
		{\small Tristan \textsc{Allwood}}\\\\[0.5cm]
		%\textbf{\small Second Marker:}
		%{\small John \textsc{Smith}}\\
		\textsc{\scriptsize Department of Computing}\\
		\textsc{\small Imperial College London}
	}

	
	\date{}
	\maketitle

\tableofcontents

\chapter{Introduction}

Software testing is a cornerstone of software engineering --- one of the most common and effective ways to verify software quality and an effort that accounts for at least 50\% of software development time \cite{tahbildar2automated}. With the fast-paced growth of the software industry, comes the need to test larger and more complex software on an unprecedented scale. Moreover, as software becomes increasingly ubiquitous, it is held to the highest standards of reliability and correctness, which further justifies testing it in a rigorous and exhaustive manner.

As a result, many attempts have been made to automate the testing effort, so that programs can be systematically and seamlessly tested, without requiring laborious, costly and error-prone manual input. The consequences of automated testing are very appealing: it reduces software maintenance and development costs, while increasing the robustness and ultimate quality of the software. Despite the fact that this area of research has taken time to develop, due to the intrinsic complexities of automatic test generation, it has now seemingly reached a stage where it can start to make a meaningful impact on software testing practice.

% have been left on the side line  s/ out of the equation ?
Decades of research have been devoted to automatic test generation for static languages and a multitude of tools have been developed. As the research area matures, it is arriving to a point where its techniques are no longer simply applicable to restricted programming language subsets or limited programs. Indeed, companies such as Microsoft employ automatic test generation tools on a regular basis to verify their software \cite{păsăreanu2009survey}. Yet, until very recently, dynamic programming languages had been left out of the equation --- but their increasing popularity and a renewed interest in them prompts the need to start including them in the automatic testing research effort.

One such programming language that has been growing in popularity in the past few years is JavaScript, with new frameworks and libraries being released for it frequently. Software libraries that have gained wide acceptance, like \texttt{Node.js}\footnote{http://nodejs.org/} which supports the writing of highly-scalable internet applications, seem to confirm JavaScript's transition from a purely client-side browser language to an all-purpose one --- at least for some. In recent studies \cite{website:langpop}, JavaScript appears amongst the most used programming languages in the world today. In other words, it seems that JavaScript is here to stay, at least for some time, and it makes sense to devote time to it.

Various test generation approaches exist: from straightforward but limited random generation to elaborate systems that combine static and dynamic analysis to provide strong software verification. Since much of the literature on automatic test generation focuses on static procedural languages and numerical input data types, many of the techniques found are not feasible or applicable to automatic test generation for JavaScript. However, some are more appropriate than others for our objective --- to generate structural unit test suites for JavaScript objects --- and it is those methods that we will explore during the course of this project.\\

This project is challenging for several reasons. First of all, it requires automatic test data generation, which is a challenging task in itself. Taking a random approach to the data generation problem is often not satisfactory, as the coverage achieved tends to be fairly poor. However, both the static and dynamic solutions that aim for a more systematic testing face major hurdles. For the static approach it is mainly to do with solving the constraints responsible for generating the test data, as this problem can become undecidable under certain circumstances. The dynamic approach depends on the execution of the program under test, and the number of executions needed for sufficient coverage can become infeasible. On top of these challenges that are common to most automatic test case generation initiatives, Flycatcher raises additional difficulties due to the fact that it targets not only an object-oriented language but a dynamic one. For instance, our testing tool will need to tackle issues such as method call sequence generation and dynamically-typed object instance generation, the latter rendered difficult by the absence of static types.

By investigating and bringing together the most relevant techniques, we believe that we can overcome the challenges encountered and build a tool for the structural unit-testing of JavaScript programs. The major contribution that this project hopes to make to the field is to extend the limited amount of work done regarding dynamic languages by proposing a tool for automatic unit test generation for, possibly restricted, JavaScript programs. As well as offering a potential tool to aid JavaScript developers, we hope that our work will be able to offer new insights into automatic test generation for a dynamic, object-oriented language and benefit future research in that direction.

%Notes on \cite{mcminn2004search}
%\begin{itemize}
%\item testing major part of the development process, crucial for quality and accounts for 50\% or more of the development effort
%\item manual process is tedious, costly and difficult and the testing is often biased
%\item exhaustive enumeration of inputs is infeasible for a reasonable program, coverage gives a good indication of test quality but full coverage is often infeasible too therefore ideally ATDG should achieve the best possible coverage
%\item random methods are easy to implement but unreliable and unlikely to discover deep errors
%\item size and complexity of real software means a lot of the research deals with toy examples/language subsets. ATDG `undecidable' problem.
%\item metaheuristic search techniques use an \emph{objective function} that estimates the value of a solution
%\item the execution tree of most programs is infinite \cite{king1976symbolic}
%\item as more and more people rely on software it will be required to be of greater quality
%\end{itemize}

%\section{Motivation}
%\section{Context}
%\section{Aim}
%\section{Contributions}
%\section{Report Structure}

\chapter{Background}
In this chapter we will start by giving an overview of software testing, with particular emphasis on aspects of it that are relevant to this project. We will then take a look at the state of the art in automatic test data generation (ATDG) in order to understand the approach that will be used for Flycatcher. Because the tests will be object-oriented, method call sequences also need to be generated for the test cases and we will look at the state of the art for doing that too. Finally we will explain why we chose JavaScript as our target language and describe features of it that are of interest to us for this project.

\section{Dynamic software testing}

\subsection{Overview}

We can define the activity of dynamic testing as testing that requires execution of the software with test data as input \cite{mahmood2007systematic} and characterise it with respect to three parameters namely the amount of knowledge assumed by the tester, the target of the tests and the stage of development at which they are executed. The amount of knowledge of the software under test can be divided into three categories, structural (white-box testing) testing, functional (black-box testing) and a hybrid of the two (grey-box testing). The target of the tests refers to their granularity, from testing specific units of code (unit testing) to an entire integrated system (system testing). The stage at which the tests are undertaken determines whether they are regression tests, alpha-tests, beta-tests, acceptance tests \emph{etc}. With Flycatcher we hope to generate suites of structural tests, focused at the unit level of object-oriented classes, most likely to perform incremental regression testing. Hence, structural testing, unit testing and regression testing will be described in more detail in this section.

\subsection{Structural testing}

The goal of structural testing is to test the internal workings \cite{mcminn2004search} of an application in order to verify that it does not contain errors. While functional testing determines whether the software provides the required functionality, structural testing tries to ensure that it does not crash under any circumstances, regardless of how it is called. It concerns \emph{how} well the software operates, its structure, rather than \emph{what} it can do, its function. As a result, the measure to determine good structural testing is the code covered during the testing process --- code coverage. It gives us an idea of the amount of code that should be bug free. However, there are various types of code coverage criteria and the confidence that our code is bug free varies depending on which one is chosen.

\subsubsection{Test coverage}
Edvardsson lists the most cited criteria \cite{edvardsson1999survey}, from weakest to strongest:
\begin{itemize}
	\item \textbf{Statement Coverage} Each statement must be executed at least once.
 	\item \textbf{Branch\footnote{For an explanation of program analysis terminology such as branch, path and control flow graph see Appendix A.}/Decision Coverage} Each branch condition must evaluate to true and false.
 	\item \textbf{Condition/Predicate Coverage} Each clause within each branch condition must evaluate to true and false.
 	\item \textbf{Multiple-condition Coverage} Each combination of truth values of each clause of each condition must be executed.
 	\item \textbf{Path\footnotemark[\value{footnote}] Coverage} Each path in the control flow graph\footnotemark[\value{footnote}] must be traversed.

The stronger criteria of condition, multiple-condition and path coverage are often infeasible to achieve for programs of more than moderate complexity, and thus branch coverage has been recognised as a basic measure for testing \cite{edvardsson1999survey}.
\end{itemize}

\subsection{Unit testing}

Unit testing consists in testing individual and independently testable units of source code \cite{myers2011art}. Therefore, unit testing is made easier if the code is designed in a modular way. The nature of the units depends on the programming language and environment but is often a class or a function. As opposed to system tests which can be aimed at the client, unit tests are usually white-box tests. Although they do not guarantee that the overall software works as required, they give confidence in specific units of code and narrow down errors, helping the development process. In Flycatcher, the target unit will be a JavaScript prototype, which will be introduced later in this chapter.

\subsection{Regression testing}
Automatically generating structural unit tests can be of great use for regression testing. Regression testing aims to ensure that enhancing a piece of software does not introduce new faults \cite{myers2011art}. The difficulty in testing this is that programmers do not always appreciate the extent of their changes. Hence, having a suite of unit tests with good structural coverage can reduce this problem by verifying the software in a systematic, unbiased way.

\section{Automatic test data generation}
\subsection{Overview}

As can be seen in Mahmood's systematic review of ATDG techniques \cite{mahmood2007systematic}, many classifications exist for ATDG techniques. For our purposes, the first distinction that we need to make is between white-box, black-box \cite{prasanna2005survey} and grey-box ATDG techniques, as for Flycatcher we are only interested in white-box testing. In the literature we found that white-box ATDG techniques are usually classified in two ways \cite{mahmood2007systematic, edvardsson1999survey, tahbildar2automated}.

The first concerns the target selection stage of ATDG techniques: where either paths or individual nodes that contribute to the overall coverage criterion are successively selected from the control flow graph, so that test data that respectively traverses the path or reaches the node can be generated. When specific paths are targeted, the ATDG technique is known as \emph{path-oriented} \cite{edvardsson1999survey} whereas if a node is targeted then it is \emph{goal-oriented}. When data is generated purely randomly \emph{i.e.} there is no specific target, then as part of this classification the ATDG technique is simply \emph{random}.

The other classification of white-box ATDG concerns the type of implementation: \emph{static}, \emph{dynamic} or a \emph{hybrid} of the two \cite{han2008empirical, mcminn2004search}. We will focus on the latter classification of structural testing as it governs our choice of implementation for Flycatcher. Moreover, the former concerns the path selection stage of ATDG and this step will be ignored in Flycatcher as it is in many recent ATDG techniques \cite{tahbildar2automated}. Figure ~\ref{atdg_overview} summarises what we believe is an intuitive characterisation of ATDG techniques with respect to this project and the one which will guide our choice of implementation. Many techniques can be found under each of the static, dynamic and hybrid implementation categories and we will only list the most noteworthy to us.

The choice of implementation for Flycatcher is dynamic random ATDG for our benchmark and dynamic search-based ATDG using genetic algorithms for our solution. The rest of this section will present in further detail the structural ATDG implementation categories we have chosen and the difficulties of ATDG for a dynamic language, so that we can understand the rationale behind this choice.

\begin{figure}
\hspace*{-1.6cm}
\begin{tikzpicture}[
  	event/.style={text width=2.7cm,text centered,font=\rmfamily,anchor=north},
  	edge from parent/.style={draw=black!50},
   % 	edge from parent path={(\tikzparentnode.south) -- ++(0,-0.5cm)
	%		-| (\tikzchildnode.north)},
	level 1/.style={sibling distance=4cm,level distance=1cm,
		growth parent anchor=south,nodes=event},
	level 2/.style={sibling distance=4cm},
	level 3/.style={sibling distance=4cm},
	level 4/.style={sibling distance=4cm}]

	\node {\textbf{ATDG techniques}}
		child [yshift=1cm] {
			child {node [level 1] {Functional (black-box)}
				child {node [level 2, xshift=-0.5cm] {Specification based}}
				child {node [level 2, xshift=-2cm] {Model-based}}
			}
			child {node [level 1,xshift=0.4cm] {Structural (white-box)}
				child {node [level 2,xshift=2cm] {Static}
		     	       child {node [level 3] {Symbolic execution}}}
				child {node [level 2,xshift=0.06cm] {Hybrid}
						 child {node [level 3] {Concolic execution}}}
				child {node [level 2] {Dynamic}
	    	 	      child {node [level 3,text width=2.5cm,xshift=1cm,yshift=-1cm] {Search-based}
			      		child { node [level 4,xshift=1.6cm] {Genetic algorithms}}
			      		child { node [level 4,xshift=0.4cm] {Simulated annealing}}
			      		child { node [level 4,xshift=-1.2cm] {Iterative relaxation}}
							child { node [level 4,xshift=-2.4cm] {Alternating variable}}
							child { node [level 4,xshift=-4cm] {...}}
			      }
		     	      child {node [level 3,text width=2.5cm,xshift=-0.4cm] {Random}}
				}
			}
		};
\end{tikzpicture}
\caption{Overview of ATDG techniques}
\label{atdg_overview}
\end{figure}


%We feel that the latter is more intuitive as specific ATDG techniques usually fall under one of these categories. Figure 123 summarises what we believe is the most intuitive characterisation of ATDG techniques to date and the one which will guide our choice of implementation for Flycatcher.

%Edvardssvon's classification \cite{edvardsson1999survey} is based on the idea that there are three approaches to automatic test data generation. The idea is that . Once a path is chosen it can be traversed by three methods: \emph{random}, (randomly explore the search space) \emph{goal-oriented} (target specific goals such as assertions) and \emph{path-oriented} (traverse an exact path).

\subsection{Static test data generation}
Static structural test data generation is based on information available from the static analysis of the program, without requiring that the program is actually executed \cite{mcminn2004search}. Static program analysis produces control flow information that can be used to select execution paths in order to try and achieve good coverage. The goal of ATDG is then to generate data that executes these paths.

Every time control flow branches, \emph{e.g.} at \texttt{if} statements, there is a corresponding predicate or branch condition. These predicates can be collected along a path and conjoined to form the path predicate. By solving the path predicate in terms of the input variables, we can obtain test data that executes that path. However, in order to rewrite the path predicate in terms of the input variables we need to take into account the execution of the program. Hence, for generation of test data statically a technique called symbolic execution \cite{king1976symbolic} is used.

Symbolic execution gathers constraints along a simulated execution of a program path, where symbolic variables are used instead of actual values, such that the final path predicate can be rewritten in terms of the input variables. Solving the resulting system of constraints then yields the data necessary for the traversal of the given path \cite{king1975new, king1976symbolic}. There are a lot of technical difficulties associated with symbolic execution \cite{edvardsson1999survey,meudec2001atgen,mcminn2004search}:

\renewcommand{\labelitemi}{\tiny$\blacksquare$}

\begin{itemize}
	\item the presence of input variable dependent loops can lead to infinite execution trees\footnote{the execution paths followed during the symbolic execution of a procedure \cite{king1976symbolic}} as the loops can be executed any number of times
	\item array references become problematic if the indexes are not constants but variables, as is typically the case
	\item features such as pointers and dynamically-allocated objects that rely on execution are hard to analyse statically
	\item static analysis is not possible for function calls to precompiled modules or libraries
	\item if the path constraint is non-linear, solving it is an undecidable problem
	\item even if the path constraint is linear, solving it can lead to very high complexity
\end{itemize}

Although various static solutions have been proposed for these issues \cite{ramamoorthy1976automated,goldberg1994applications,offutt1999dynamic}, they often dramatically increase the complexity of the ATDG process. As a result, tools purely based on symbolic execution can typically handle only subsets of programming languages and are not applicable in industry. A better trend that has developed in the past decade, is the combination of concrete and symbolic execution, which tackles most of the aforementioned issues \cite{păsăreanu2009survey} --- we will cover this type of ATDG implementation in the subsection on hybrid ATDG. Due to the numerous problems posed by purely static ATDG, its weakness with dynamic types and constructs \cite{edvardsson1999survey,tahbildar2automated} and the complexity of building a fully-fledged symbolic executor for a language \cite{edvardsson1999survey,han2008empirical}, we chose not to use static ATDG for the implementation of Flycatcher.

\subsection{Hybrid test data generation}

The hybrid approach to ATDG consists in combining symbolic and concrete execution, which is known as \emph{concolic execution} \cite{păsăreanu2009survey}. In other words, hybrid analysis tools run programs on actual inputs, while collecting symbolic constraints in order to direct the search for new inputs. In doing so, they avoid the main weaknesses of the static approach, such as solving non-linear constraints or dealing with dynamic structures. This type of technique has been popular in recent years, mainly because it overcame the limitations that prevented static ATDG techniques from being applied to industry software. Notable tools that implement it are DART \cite{godefroid2005dart}, CUTE \cite{sen2005cute}, JPF-SE \cite{anand2007jpf}, PEX \cite{tillmann2008pex}, EXE \cite{cadar2008exe} and KLEE \cite{cadar2008klee}.

Yet, although it deals with some limitations of static ATDG, hybrid ATDG still requires static analysis of the source code under test, which is unfeasible for Flycatcher, given the dynamically typed, object-oriented nature of JavaScript.

\subsection{Dynamic test data generation}

Dynamic structural test data generation is purely based on actual execution of the software. The program under test is run with, possibly randomly, selected input and feedback is collected at runtime regarding the chosen coverage objective \cite{edvardsson1999survey}. The feedback is usually obtained through some form of instrumentation of the program that monitors the program flow. Inputs can be continually generated randomly, relying on probability to achieve the coverage objective --- this is known as \emph{random} test data generation and does not perform well in general \cite{edvardsson1999survey}. On the other hand, inputs can be incrementally tuned based on the feedback (using different kinds of search methods) in order to satisfy the coverage objective --- this is known as \emph{search-based} test data generation \cite{mcminn2004search}, where the search-space is the control flow graph of a program. The main drawback of dynamic ATDG is that it is reliant on the speed of execution of a program and as the number of required executions to achieve satisfactory coverage may be high, this leads to an overall expensive process. Below we will present the random and search-based approaches in more detail, in order to understand which would be more suitable for Flycatcher.

\subsubsection{Random approach}

Random test data generation consists in producing inputs at random in the hope of achieving the chosen coverage criterion through probability. Although random test data generation is relatively simple to implement, it does not perform well in terms of coverage, as the chances of finding faults that are revealed by only a small percentage of program inputs are low \cite{edvardsson1999survey}. In other words, it is difficult for it to exercise `deep' features of a program that are exercised only through specific and unlikely paths. As a result, random ATDG only works well for straightforward programs. However, because it is the simplest ATDG technique and is considered to have the lowest acceptance rate \cite{edvardsson1999survey}, it is often used as a benchmark and is a suitable candidate for us to benchmark our Flycatcher application.

\subsubsection{Search-based approach}
% more on search-based approach using mairhofer's stuff?

Search-based test data generation uses heuristics to guide the generation of input data so that the inputs execute paths that contribute to the overall test coverage objective. This involves modelling the test coverage objective as a heuristic function or \emph{objective function}, that evaluates the fitness of a chosen set of inputs with respect to a coverage objective. Based on those fitness values, many search techniques exist to find optimal inputs in order to achieve the desired coverage. Various objective functions exist and they are dependent on the ATDG method used. Some of the well-known search-based ATDG techniques are \emph{alternating variable} (local search optimisation) \cite{korel1990automated, gallagher1997adtest}, \emph{simulated annealing} \cite{tracey1998automated,tracey1998way}, \emph{iterative relaxation} \cite{gupta1998automated} and \emph{genetic algorithms} \cite{michael1998automated,michael2001generating}.
	
In 2008, Han and Kwon \cite{han2008empirical} conducted a robust empirical evaluation of search-based test data generation techniques, comparing iterative-relaxation, local search optimisation and genetic algorithms. Genetic algorithms came out on top of the other techniques regarding the rate of coverage and generality, both essential characteristics when considering ATDG techniques. However, genetic algorithms proved expensive both in time and resources, but this can be improved upon \cite{han2008empirical}.
Although simulated annealing is not part of this empirical evaluation, we can see from \cite{michael1998automated} that although it performs as well as genetic algorithms, it appears to be less efficient.

Additionally, genetic algorithms offer another advantage: as opposed to most other ATDG techniques, they do not require static analysis of the program under test \cite{han2008empirical}. This advantage is especially significant in our case, as to the best of our knowledge, tools that produce control and data flow analysis information for JavaScript source code are not available. However, this is not surprising since the dynamic, object-oriented nature of JavaScript makes this task very laborious.

Consequently, due to their effectiveness in terms of coverage and the advantages they possess with regard to dynamic languages, genetic algorithms appear to be the most suitable for our implementation of ATDG for Flycatcher. We will therefore go on to explain what genetic algorithms are and how they can be applied to test data generation.

\subsection{Genetic algorithms}

\subsubsection{Overview}
Genetic algorithms attempt to model the robustness and flexibility of natural selection in order to guide a search\cite{michael2001generating}. They start with a randomly initialised \emph{population} of potential candidate solutions, called \emph{individuals} or \emph{chromosomes}. The population is iteratively recombined and mutated to evolve successive populations, known as \emph{generations}. The recombination takes the `fittest' parent solutions and `breeds' them to produce new offspring. Their fitness is determined by a \emph{fitness function} that evaluates how good a candidate solution is for a particular problem.

This process favours evolution towards fitter individuals, mimicking natural selection. The `breeding' of two individuals usually involves a crossover operation which swaps their information at a randomly selected position. In the interest of maintaining diversification, a \emph{mutation} phase usually occurs after the crossover, to introduce new genetic material into the search and avoid premature convergence on one area of the search space. The mutation operation randomly modifies some information of a selected individual, The process of iterative recombination and mutation, illustrated in Figure \ref{ga}, is repeated until a specific termination criterion is fulfilled.

\begin{figure}
\hspace*{0.7cm}
\centering
\includegraphics[scale=0.7]{ga.pdf}
\caption{Flowchart of a genetic algorithm}
\label{ga}
\end{figure}

\subsubsection{Application to test data generation}
\label{subsubsec:ga_application}
For the purposes of ATDG, the population consists of input test data. The data evolves according to the genetic algorithm in order to satisfy the chosen coverage criteria \cite{michael2001generating}. In other words, the \emph{fitness function} is based on the \emph{objective function}, the function that evaluates test data according to a certain coverage criterion --- the more a candidate contributes to coverage, the fitter it is. Instead of using search heuristics to minimise our objective function, we use it to guide our natural selection process towards the data that will achieve the best coverage.

Regarding the objective function used in the context of genetic algorithms, different variants exist \cite{mcminn2004search}. The \emph{coverage-oriented approach} rewards individuals on the basis of \emph{all} covered program structures. This type of objective function rewards coverage with respect to the overall goal \emph{i.e.} the system is not required to pick a specific path or node and attempt to execute it. The other approach is \emph{structure-oriented} coverage where, similarly to most other search-based ATDG methods, a structure (\emph{e.g.} a path or a node) is selected from the data flow analysis and a separate search is undertaken for each selected structure. Once more, seeing that the \emph{coverage-oriented approach} does not require static analysis of the program under test, it is the most attractive to us for the implementation of Flycatcher.

Finally, the population, individual representation, fitness evaluation and methods of crossover, mutation and selection are all highly dependent on the test data generation problem at hand, hence we will discuss them with regard to Flycatcher when we come to the implementation.

%For a simple procedural language the evolution process will deal with test data whereas for object-oriented tests, the recombination and mutation could concern call sequences and types.

%We believe that obtaining the best coverage for possibly complex programs is worth using GA. GA does not 

% tends to be as effective in terms of coverage, the simulated annealing method requires more executions than the GA method and is therefore less desirable. Additionally -> less work for GA implementation = no static analysis information needed? no instrumentation needed? 

%The iterative-relaxation method is part of the hybrid approach and we will look at this type of approach later in the next section. Regarding the search-based methods which are part of the dynamic approach to ATDG,

%through the program execution search-space in order to achieve the chosen test coverage criteria.

\subsection{Challenges of dynamic object-oriented languages}

Most of the research on ATDG concerns static programming languages \cite{mahmood2007systematic} and it is only in the past few years that dynamic programming languages have sparked some interest in that field. A possible reason for this is that dynamic programming languages make ATDG harder by enabling features that allow programs to significantly change at runtime. These features can include modifying the type system, extending objects or adding new code during program execution. The challenges this type of behaviour introduces, and that Flycatcher will try and overcome, are listed below \cite{ducasse2011challenges}.

\subsubsection{Generating test data of the required type}
Given that method parameters do not have static types in dynamically typed languages, we do not know what arguments to pass to them. A potential solution to this is to use a method called \emph{type inference} \cite{pluquet2009fast}, which tries to infer the type of arguments from the way they are used inside the program. Although this method does not guarantee 100\% precision, it is a good starting point for generating accurately typed test data. Mairhofer uses this technique for \textsc{\small RuTeG} \cite{mairhofer2008search}, his search-based ATDG tool for Ruby, where the search for test data refines the initially inferred type, by discarding poor candidates.

\subsubsection{Generating object instances}
Sometimes the type of an object constructor or method parameters will be a complex type and this complicates the test data generation task even further. Generating well-formed object instances to use as arguments inside tests for a dynamically typed object-oriented language is problematic because there isn't a blueprint to construct them from. There is previous work on input data generation for dynamic data structures \cite{korel1990automated, visvanathan2002generating, sai2005address, zhao2007automatic}, but all these approaches focus on statically typed languages (C/C++), require static program analysis and mostly lack generality.

Another approach uses something called needed-narrowing \cite{antoy1994needed} or lazy instantiation \cite{lindblad2007property} --- where instances are only created when they are actually put to use by the program. This enables test data generators to adjust object instances during execution, when attempts are made to use them, so that they always have the required type. This technique is used by \textsc{Irulan} \cite{allwood2011high} for generating tests in Haskell, which has lazy evaluation by default. For the purpose of complex type test data generation in Flycatcher, we will investigate whether we can mimic lazy evaluation for JavaScript objects in order to build test data with the appropriate complex type.

\subsubsection{Identifying bugs}

In dynamic languages such as JavaScript, the function signatures bear no type information and this makes it difficult to know whether an exception is raised due to a wrongly typed test argument or a true program bug. In the case where the exception is not a bug it could be due to two things: manipulating a badly initialised object or breaking a program precondition. The bad initialisation problem might be prevented through the use of lazy evaluation to initialise objects. The breaking of preconditions could be avoided by giving the tester the ability to impose restrictions on the test data generator, so that preconditions for the program are respected. Being able to identify real program bugs during test data generation is an issue that will need further attention during the implementation of Flycatcher.

\subsubsection{Dealing with dynamically generated code}

Dynamic languages sometimes offer features that parse and evaluate a string at runtime and execute it as code, such as JavaScript's \texttt{eval} function. However, not only are these features potentially insecure, they make any analysis for test data generation much harder. As the general use of \texttt{eval} in JavaScript is prohibited anyway, we can safely ignore it for the purpose of Flycatcher.

\section{Object-oriented test case generation}
Most of the research on test generation focuses on testing imperative functions, such that the automated generation required is that of the functions' input parameters. However, when dealing with object-oriented code, a different approach is needed, as the unit under test changes from a function to an object. To test one of an object's methods, three steps are necessary \cite{tonella2004evolutionary} and should be repeated until the chosen coverage criterion for the method under test is met.
\begin{enumerate}
	\item Instantiate the object
	\item Call some of its methods to possibly modify its state
	\item Assert that the method under test returns the expected answer
\end{enumerate}

Because it is impossible to know how the application will use the class/object under test in reality, as many relevant test cases as possible must be tried in order to maximise the likelihood of finding a bug inside the class in question. Test coverage, the assessment measure that is used for our test case generation problem, is a good indicator of the relevance of test cases for a particular class and method under test. Hence, this measure can be used, as for the generation of input \emph{data}, as a search objective to guide the generation of \emph{method call sequences} for our test cases. Tonella was one of the first to use search-based methods for the generation of adequate OO test cases, using genetic algorithms as the search heuristics method \cite{tonella2004evolutionary}. Indeed, the procedure using genetic algorithms to generate input test data for Flycatcher described in ~\ref{subsubsec:ga_application} can simply be extended to the generation of object-oriented tests by adapting individuals so that they represent their structure.

The code example below illustrates the type of object-oriented structural unit test that we aim to generate with Flycatcher (assuming a standard linked list implementation):

\begin{verbatim}
l = new LinkedList
l.add(true)
l.remove(true)
l.add(true)
assert (l.size == 1)
\end{verbatim}

where \texttt{LinkedList} is the class under test (a JavaScript prototype) and \texttt{size} is the method under test.

%, as well as their inputs and the inputs to the constructor, should all evolve in a way that permits a successful coverage to be achieved. Automatic test \emph{data} generation techniques that maximise coverage have been discussed, but work has also been done on techniques to the evolution of object-oriented tests cases.

%One thing to note about such a test case is that it does not mean the method under test is bug-free --- again, the unit under test is the object, therefore many instantiations and method call sequences need to be tried
%One element of this procedure that we have not discussed so far is the generation of a sequence of method calls for the second step.

%create a sequence of methods leading to the method under test, it is the method under test which gives an assertion using its return value

\section{JavaScript}
%\subsection{Prominence}
\subsection{Idiosyncratic features}


\subsection{Object-oriented programming}

\chapter{Project plan}

\section{Outline}
\label{sec:project_outline}
The plan for Flycatcher can be divided into roughly eight stages which are outlined below.

\begin{enumerate}
	
	\item{\textbf{Program Analyser}} The Program Analyser component is responsible for retrieving information about the program that is available statically and can be used to generate the test cases. It receives a class or object under test and retrieves the object's methods as well as their signatures. Given that there are no static types in JavaScript, an additional step is needed to try to infer input variables' types from their use in the respective methods. The information collected by the program analyser will be used to create individuals to represent test cases for a particular method under test later on, during the implementation of the genetic algorithm. \emph{This stage of the project is already partially completed.}
	 
	\item{\textbf{Random \emph{Data} Generators for primitive types}} Given the added difficulty of generating and initialising object instances to pass in as arguments to the various methods present in our test cases, these will be dealt with at a later stage. At this stage, random data generators will be developed for three of JavaScript's primitive types namely \emph{Boolean}, \emph{Number} and \emph{String}.

	\item{\textbf{Test Case Executor}} In order to determine which generated test cases are worth keeping because they are appropriate for the a particular method under test \emph{or} if they contribute to test coverage, an execution environment must be developed. The goal of this environment is to track the coverage achieved by a series of generated test cases, dismissing test cases that are not yet adequate for the method under test or do not contribute to overall coverage. Developing this environment will most likely involve instrumenting programs under test with code to track the coverage achieved. Depending on their feasibility, either statement or branch coverage\footnote{it is worth noting that branch coverage is the stronger alternative of the two, although its implementation may be more involved} will constitute the coverage criterion for Flycatcher and will be tracked by the Test Case Executor.

	\item{\textbf{Random \emph{Test Case} Generator}} This step involves building a component that generates unit test cases randomly. The test cases will consist of individuals: a constructor call, a method call sequence ending with the method under test and the appropriate input data at each stage. The test cases that result from this module are bound to achieve poor coverage due to the low probability of exploring deep paths. Hence, this component will be used to benchmark the Flycatcher application.
	
%Coverage feedback from the Test Case Executor will be used, not to improve the test cases as they are intrinsically random, but as a possible alternative to a time limit as a stopping criterion for the generation of tests --- the random approach is unlikely to achieve 100\% coverage and needs a condition to stop or it will run forever.

	\item{\textbf{Search-based \emph{Test Case} Generator}} This is probably the core module of the implementation: it consists in generating test cases that strive to achieve good coverage by using heuristic search methods to guide the test case generation. The search heuristics will be effective on two levels: they will direct the generation of \emph{data} for input parameters but also the generation of \emph{method sequences} to be called on the object under test. On top of its evaluation purposes, the coverage feedback from the test case executor will be used by the Search-based Test Case Generator to guide the search for quality input data.

	\item{\textbf{Dealing with complex types}}
At this stage we will introduce JavaScript complex types (\emph{Array} and \emph{Object}) as potential input parameters in our test generation tool. In an attempt to mimic the concept of lazy instantiation, complex types will be created empty and built up as required by the program. In the case of the Search-based Test Case Generator, the search heuristics will contribute to the creation of object instances of the correct dynamic types (as it did for the creation of correct primitive types).
		
	\item{\textbf{Extensions}} Currently, two possible extensions are envisaged for this project. The first one concerns the introduction of custom input data generators, such that testers can adapt the test data generation to their specific needs in order to achieve better code coverage. Secondly, given that we will use a dynamic approach which comes with a potentially high execution time, another extension would be to investigate and implement methods of speeding up the implementation of Flycatcher. Other extensions are of course possible and may come to light later on during the project.
		
	\item{\textbf{Evaluation}}
	The final stage will consist in evaluating the success of our project and its contribution to the state of the art. The next subsection will describe the intended evaluation process in more detail.

\end{enumerate}

\section{Evaluation}

An evaluation of our system will need to be conducted towards the end of the project, involving experimentation with our automatic test generation tool. The random test case generation will be used as a benchmark for Flycatcher, which uses a search-based solution in order to achieve better coverage. To start off with, experiments will be run on a set of overall relatively straightforward programs of varying size, complexity and number/type of parameters. If the implementation of Flycatcher permits it, we will attempt to use our tool to test larger, real-life software such as JavaScript libraries.

In order to constitute a success, Flycatcher would have to generate unit tests suite for, possibly restricted, JavaScript programs written in an object-oriented style \emph{i.e.} treating JavaScript \emph{prototypes} like classes in the traditional OO paradigm. Additionally, it should perform better using search-based test case generation than random generation and achieve reasonable coverage (statement or branch) compared to other similar applications. We will compare the performance of our tool to applications such as \textsc{RuTeG} which bear resemblances to it, in order to assess its performance from a more general perspective. Overall, the consistent test coverage achieved for programs of various complexities will be the main criterion used in order to evaluate Flycatcher and its contribution to the state of the art in automatic test case generation.

\section{Time-frame}
Our aim is to complete at least steps 1 to 4 in section ~\ref{sec:project_outline} by the end of the spring term. During the spring holidays, not much progress will be made due to a focus on examinations. Hence, of the five weeks remaining after the examination period, the first two will be aimed at completing the section that deals with complex input data types (if it is not finished already), evaluation and looking at possible extensions if time permits. The three final weeks before project submission will be entirely devoted to putting the final report together.

\bibliographystyle{acm}
\bibliography{interim}
\end{document}

