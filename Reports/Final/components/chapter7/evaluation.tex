\chapter{Evaluation}
In this chapter, we evaluate \textsf{Flycatcher} quantitatively by running \textsf{Flycatcher} with a series of benchmark programs and observing the results. In this experimental evaluation we are interested in examining some of \textsf{Flycatcher}'s features as well as assessing its overall success in terms of code coverage. We start by introducing the choice of benchmark programs, before presenting the experiments and finally discussing them.

% by using it to generate unit tests for a chosen set of programs and examining the results.
% First, we present a summary of the parameters available to \textsf{Flycatcher} users. We then explain our rationale and constraints in choosing the benchmark programs. Next, we describe the experiments and present the results for each program before discussing the results obtained.

\section{Choosing the benchmark suite}
In choosing a suite of programs for evaluating \textsf{Flycatcher} we had the following objectives in mind:

\begin{itemize}
   \item Demonstrating \textsf{Flycatcher}'s ability to infer primitive types
   \item Demonstrating \textsf{Flycatcher}'s ability to infer user-defined types
   \item Demonstrating that \textsf{Flycatcher} works with methods of various size, complexity and arity
   \item Demonstrating \textsf{Flycatcher}'s ability to achieve high coverage for various methods in a reasonable time
\end{itemize}

However the constraints imposed to us by the current limitations of the \textsf{Flycatcher} application, made finding benchmark tests difficult. The constraints are the following:

\begin{itemize}
   \item \textsf{Flycatcher} does not handle the inference of \texttt{Array} parameters
   \item \textsf{Flycatcher} does not handle the inference of \texttt{Function} parameters
\end{itemize}

The reason why these constraints made it difficult to find benchmark programs is that array parameters are commonplace and because functions are first-class objects in JavaScript, they frequently occur as parameters too. This also indirectly imposed a restriction on the size of the benchmark programs --- most significant libraries and modules make use of either arrays or functions as parameters at some point. This is also why established JavaScript benchmark suites such as SunSpider or the V8 benchmark suite couldn't be used.

% Nevertheless, we were still able to evaluate whether we accomplished what we initially set out to do: build an automatic test generator for a comprehensive \emph{subset} of the JavaScript language.
With these objectives and constraints in mind, the list of methods in table \ref{benchmarktests} was put together. Some of the programs are custom, many were found using \textsf{Node.js}'s open-source module registry \textsf{npm} and others were taken from the V8 performance benchmark suite.

\begin{table}[h]
\centering
\begin{tabular}{|l|l|c|c|}
\hline
\textbf{Class} & \textbf{Method} & \textbf{LOC} \\
\hline
\texttt{Triangle} & & \\
           & \textit{getType} & 38      \\
\hline
\texttt{LinkedList} & & \\
           & \textit{append} & 15       \\
           & \textit{remove} & 16       \\
           & \textit{prepend} & 12      \\
           & \textit{insertAfter} & 8   \\
           & \textit{insertBefore} & 8  \\
           & \textit{at} & 6            \\
\hline
\texttt{BinarySearchTree} & &     \\
           & \emph{add} & 49      \\
           & \emph{contains} & 26 \\
           & \emph{remove} & 147  \\
           & \emph{size} & 9      \\
\hline
\texttt{RedBlackTree} & &         \\
           & \emph{insert} & 22   \\
           & \emph{contains} & 14 \\
\hline
\texttt{SplayTree} & &             \\
           & \emph{splay} & 60 \\
           & \emph{insert} & 23 \\
           & \emph{remove} & 23 \\
           & \emph{findMax} & 10 \\
\hline
\texttt{Luhn Algorithm} & &     \\
           & \emph{isValidIdentifer} & 40 \\
\hline
\texttt{Base 64} & &     \\
           & \emph{encode} & 45 \\
           & \emph{decode} & 50 \\
\hline
\texttt{SHA1} & &             \\
           & \emph{hash} & 72 \\
\hline
\texttt{Poker} & &                 \\
           & \emph{rankHand} & 437 \\
\hline
\end{tabular}
\caption{Benchmark methods}
\label{benchmarktests}
\end{table}

% \begin{table}[h]
% \centering
% \begin{tabular}{|l|l|c|}
% \hline
% \textbf{Class} & \textbf{Method} & \textbf{LOC} \\
% \hline
% \texttt{Triangle} & & \\
%            & \textit{getType} & 38      \\
% \hline
% \texttt{LinkedList} & & \\
%            & \textit{append} & 15       \\
%            & \textit{remove} & 16       \\
%            & \textit{prepend} & 12      \\
%            & \textit{insertAfter} & 8   \\
%            & \textit{insertBefore} & 8  \\
%            & \textit{at} & 6            \\
% \hline
% \texttt{BinarySearchTree} & &     \\
%            & \emph{add} & 49      \\
%            & \emph{contains} & 26 \\
%            & \emph{remove} & 147  \\
%            & \emph{size} & 9      \\
% \hline
% \texttt{RedBlackTree} & &         \\
%            & \emph{insert} & 22   \\
%            & \emph{contains} & 14 \\
% \hline
% \texttt{SplayTree} & &             \\
%            & \emph{splay} & 60 \\
%            & \emph{insert} & 23 \\
%            & \emph{remove} & 23 \\
%            & \emph{findMax} & 10 \\
%            & \emph{findGreatestLessThan} & 17 \\
% \hline
% \texttt{Luhn Algorithm} & &     \\
%            & \emph{isValidIdentifer} & 40 \\
% \hline
% \texttt{Base 64} & &     \\
%            & \emph{encode} & 45 \\
%            & \emph{decode} & 50 \\
% \hline
% \texttt{SHA1} & &             \\
%            & \emph{hash} & 72 \\
% \hline
% \texttt{Poker} & &                 \\
%            & \emph{rankHand} & 437 \\
% \hline
% \end{tabular}
% \caption{Benchmark methods}
% \label{benchmarktests}
% \end{table}

\subsection{Triangle types}
The \texttt{Triangle} example is used in many testing papers for automatic test generation. It takes three numerical inputs and determines whether they can form a triangle. If so, it returns the type of the triangle \emph{i.e.} whether it is equilateral, isoceles or scalene. This example was chosen because it demonstrates \textsf{Flycatcher}'s ability to narrow down the search space for good test programs, by using the same parameters more than once inside the generated tests. Without that ability, generating three numerical inputs out of the set of natural numbers so that they form an equilateral triangle would be extremely inefficient. No custom data generators with this class in the experiments.

\subsection{Doubly circular linked list}
The doubly circular linked list example was picked because it demonstrates \textsf{Flycatcher}'s ability to infer user-defined types as well as deal with parameters which are not used in the program under test itself. The circularity of this data structure also shows that \textsf{Flycatcher} handles scenarios where termination issues might arise in the context of test generation. The implementation used is from \textsf{computer-science-in-javascript}\footnote{"\url{https://github.com/nzakas/computer-science-in-javascript}"}.

\subsection{Binary trees}
A variety of binary trees were chosen as benchmarks because they present interesting control structures for code coverage, as well as the requirement that they infer a user-defined type: the type for the trees' nodes. The standard \texttt{BinarySearchTree} implementation is from \textsf{computer-science-in-javascript}\footnotemark[2]. The \texttt{RedBlackTree} (a self-adjusting BST) implementation is from \textsf{red-black-tree-js}\footnote{"\url{https://github.com/jeffreyolchovy/red-black-tree-js}"}. The \texttt{SplayTree} (a self-adjusting BST with quick retrieval of recently accessed nodes) implementation is part of the Google V8 benchmark suite\footnote{"\url{https://github.com/hakobera/node-v8-benchmark-suite}"}. No custom data generators were used with these programs.

\subsection{Luhn Algorithm}
The Luhn Algorithm is a checksum algorithm used to validate the format of credit card numbers. This example demonstrates the use of custom data generators in order to narrow down the search space for code coverage. We specify the following \texttt{RegExp} as the random string generator, which represents a string of digits which may contain a character as well (to also test with \texttt{NaN}): \texttt{[0-9]+a?}. The code can be found in \textsf{computer-science-in-javascript}\footnotemark[2].

\subsection{Base 64}
The \texttt{Base64} class simply encodes and decodes text strings to and from a radix-64 representation. This program was chosen to test the efficiency of code coverage of its control structures. Custom data generators were used to achieve full coverage, as non-ASCII parameters take a specific path in the \emph{encode} method, as do non-base-64 strings in the \emph{decode} method. The custom string generators are, therefore respectively, \texttt{\textbackslash w+\textbackslash u0100?} and \texttt{\textbackslash w+}.

\subsection{SHA1}
The \texttt{SHA1} algorithm generates a SHA-1 secure hash of a string. The implementation is taken from Chris Veness\footnote{"\url{http://www.movable-type.co.uk/scripts/sha1.html}"}.

\subsection{Poker}
The benchmark method with the most deeply nested structure is the \texttt{Poker} class's \emph{rankHand} method. It carries out hundreds of comparisons and calculations in order to return the absolute rank of a hand of cards in Texas Hold'em poker. The input thus has to conform to a hand of poker cards, which is defined in the program as a string of five characters. A custom string generator is thus used for that purpose, with the following regular expression: \texttt{[AKQJT98765432]\{5\}}. The code is from \textsf{node-poker}\footnote{"\url{https://github.com/mjhbell/node-poker}"}.

\section{Experiments}
All the experiments are carried out on Mac OS X v10.6 with a 2.4GHz Intel Core 2 Duo processor and 4GB of RAM. Any of the configurable parameters are either specified or their default value is used. The noteworthy default values are:

\begin{itemize}
   \item type inference delay: 20
   \item maximum sequence length: 10
   \item number generator: \texttt{\textbackslash d\{10,20\}}
   \item string generator: \texttt{\textbackslash w\{10,20\}}
\end{itemize}

\subsection{Effect of varying the type inference delay}
In this first experiment we analyse the effect of varying the type inference delay discussed in part \ref{typeinference} (see usage in section \ref{usage}). Let us recall that this delay is put in place in order for \textsf{Flycatcher} to make confident type inferences, based on enough data. If there is not enough data when type inference is attempted for a parameter, we consider that the type inference is \emph{unsuccessful}. In this trial, we observe the relation between the type inference delay and unsuccessful type inferences for the methods from the benchmarking set, using an average over twenty runs and displaying the results for two of the methods in figures \ref{trianglebench} and \ref{bstbench}.

\begin{figure}[h]
\hspace*{-0.5cm}
\centering
\includegraphics[scale=0.55]{./components/chapter7/triangle.pdf}
\caption{Results for method \texttt{Triangle.getType}}
\label{trianglebench}
\end{figure}

\begin{figure}[h]
\hspace*{-0.5cm}
\centering
\includegraphics[scale=0.55]{./components/chapter7/bst.pdf}
\caption{Results for method \texttt{BinarySearchTree.add}}
\label{bstbench}
\end{figure}

\subsection{Effect of varying the length of tests}
\label{exp2}
In the second experiment, we vary the user-configurable variable that sets the length of method call sequences in tests (see usage in section \ref{usage}). We observe what the effect of modifying this variable is for all of the methods in the benchmark set. To give us a more comprehensive set of results to analyse, this process is carried out three times with varying `strict' timeouts of: 1 second, 5 seconds and 20 seconds. The results appear in figures \ref{1s}, \ref{5s} and \ref{20s} and each entry constitutes an average over 20 runs.

\begin{figure}[h]
\vspace*{-2.5cm}
\centering
\includegraphics[scale=0.56]{./components/chapter7/1sr.pdf}
\caption{Results with 1s timeout}
\label{1s}
\end{figure}

\begin{figure}[h]
\vspace*{-2.5cm}
\centering
\includegraphics[scale=0.56]{./components/chapter7/5sr.pdf}
\caption{Results with 5s timeout}
\label{5s}
\end{figure}

\begin{figure}[h]
\vspace*{-2.5cm}
\centering
\includegraphics[scale=0.56]{./components/chapter7/20sr.pdf}
\caption{Results with 20s timeout}
\label{20s}
\end{figure}

\section{Discussion}
% \subsection{Results}
\subsection{Effect of varying the type inference delay}
From the first experiment, we can observe that there is a clear relation between unsuccessful type inferences and type inference delay\footnote{to reiterate, the type inference delay variable is expressed in terms of the minimum number of uses of a parameter before type inference}: the smaller the type inference delay, the higher the number of unsuccessful type inferences. Hence, this user-configurable parameter which reflects the confidence of the type inference estimates \emph{does} effect test generation results and thus deserves careful consideration.

As we can see from the results, the number of unsuccessful type inferences tends to zero as the type inference delay increases. This led to a choice of 20 for the default value of this variable within the application, such that by default, highly confident type estimates are made by the test generator. Although the results are displayed for only 2 of the 22 benchmark methods for conciseness, the experiment was carried out for all of the methods, with equivalent results.

\subsection{Effect of varying the maximum length of tests}
The second experiment teaches us that the length of method call sequences in generated tests also has a significant impact on test generation. First of all, in some cases we observe that if the maximum length of tests is too small, certain parts of a MUT may be \emph{unreachable}. For example, when the method sequence length is 1, the code coverage for method \texttt{LinkedList.append} stays stuck around 60\%, no matter how long the timeout.

Next, we also observe that although one may initially think that longer tests are synonymous of more efficient code coverage, the results in this experiment proves this wrong. Due to the impact of generating and running very large tests, it is \emph{not} always the case that bigger tests yield better coverage. In fact we can observe a trend among the benchmark methods whereby the code coverage peaks for lengths between 5 and 20 but drops afterwards. This is particularly observable in figure \ref{1s}, for methods \texttt{LinkedList.remove}, \texttt{LinkedList.insertAfter} or \texttt{SlapyTree.insert}.

Finally, the effect of modifying the maximum length of tests is not always predictable as can be seen in figure \ref{1s} with the \texttt{SplayTree} methods \texttt{splay}, \texttt{remove} and \texttt{findMax}. In those cases, increasing the maximum length of method call sequences has a varying effect on coverage efficiency. It is not exactly clear what the best approach is to get the most efficient code coverage.

All of the above observations point to the same fact: finding the optimal length of tests for efficient coverage of a MUT is not only \emph{necessary} for performance, but its unpredictability means that it is no task for humans. This leads us to one of the clear limitations of \textsf{Flycatcher}: the randomness of the test generation process, particularly when it comes to the length of tests, is restrictive.

\subsection{Code coverage}
Experiment in section \ref{exp2}, on top of showing the effect of the length of tests, revealed \textsf{Flycatcher}'s success in terms of overall code coverage. Table \ref{covresults} shows a summary of the proportion of methods for which full coverage is achieved. The results are highly promising: only 2 out of the 22 methods are not fully covered in under 20 seconds and 100\% coverage is reached for 14 of them in under a second.

In structural testing, the major indicator of the quality of a test is the amount of code coverage achieved, which by extension is also a sound indicator of the quality of a test generation tool. It would thus be fair to say that full coverage for 91\% of the methods in a benchmark is evidence of a successful test generation tool.

\begin{table}[t]
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Timeout} & \textbf{100\% coverage} \\
\hline
1s & 14/22 \\
5s & 20/22 \\
20s & 20/22 \\
\hline
\end{tabular}
\caption{Proportion of methods with full coverage}
\label{covresults}
\end{table}


\subsection{Limitations}
Despite producing satisfying results with the chosen benchmark suite, \textsf{Flycatcher} presents certain limitations. A major design decision behind \textsf{Flycatcher} was to make the tool as autonomous and convenient as possible. To a certain extent, this has been achieved, as the minimal input a tester has to provide to get a suite of unit-tests for an entire class is:

\begin{itemize}
   \item the name of the file with the class
   \item the name of the class
\end{itemize}

In the best case scenario, providing only those two arguments will supply the tester with a full suite of unit tests for the class as well as a log of the tests that failed. However, this level of autonomy does have its drawbacks: by entrusting decisions such as the optimal length of tests or the seed for data generators to \textsf{Flycatcher}, the tester might lose on efficiency or accuracy. The high overall accuracy and efficiency of the results in the experiments in this chapter owe in part to the explicit specification of the maximum length of tests and custom data generators. For example, the method \texttt{Poker.rankHand} cannot achieve \emph{any} coverage under 20 seconds by using the default string generator, as it expects a particular string format --- the odds are simply too low. However, because we \emph{do} value autonomy in such a tool, as we know how much programmers prefer programming to testing, we feel that the solution is not to ask more input from the user, but to make \textsf{Flycatcher} better. In other words, \textsf{Flycatcher} is restricted by its use of random test generation, be it for data itself or the test cases. Although this can be partly remediated with input from the user, as in the experiments, in order to be meaningfully autonomous without compromising accuracy or efficiency, \textsf{Flycatcher} needs to become more `clever' in its exploration of the search space of possible tests going forward.\\

Another major limitation of \textsf{Flycatcher} is the lack of support for the standard objects \texttt{Function} and \texttt{Array}. Arrays are commonplace function parameters in programs and in JavaScript, so are function objects. As a result, not handling these constructs imposes strong limitations on the programs that can be tested by \textsf{Flycatcher}. This was evident in the benchmark suite selection stage, where we were precluded by this limitation from using any of the established benchmark suites or substantial JavaScript libraries. This in turn represents a threat to the validity of our results as we face a selection bias --- the lack of support for \texttt{Array} and \texttt{Function} impacts the size of programs that can be tested. Because they have such implications, we feel that it is worth outlining why supporting these types is a challenge.

\textbf{\underline{Functions}}: When automatically generating tests, all we have to begin with is the program under test. With regard to a function parameter, this is what can \emph{possibly} be learnt from the program:

\begin{enumerate}
   \item finding out that it is a function (initially)
   \item what types its parameters belong to
   \item what type its return value belongs to
\end{enumerate}

However, as far as we understand, it is not possible to determine what the function in question is or does from this information.

\textbf{\underline{Arrays}}: Arrays are known to pose problems in automatic test data generation \cite{tahbildar2automated}. In \textsf{Flycatcher}, the fact that we use proxies which return random values when called with a primitive operator, means that we cannot have `faith' in any of the primitives inside the program while generating tests. This makes it difficult to discover the length of arrays. Moreover, because arrays allow for heterogenous types in JavaScript, every entry has to be inferred and generated separately, which adds to the challenge and is likely to be an expensive process.

\textsf{Flycatcher} also does not currently support other standard types for parameters such as \texttt{RegExp} or \texttt{Date} but these have less serious implications and their implementation poses less problems.\\

In summary, \textsf{Flycatcher} is limited by the fact that it uses random test generation. Despite that this can be alleviated by input from the user, the fact remains that the application would benefit from using search-based heuristics to guide the exploration for test cases. Moreover, the lack of support from certain crucial types of programming constructs, albeit for good reasons, means that \textsf{Flycatcher} can only be used with a certain class of programs, and thus lacks generality.

% talk about the problem of not having Fuction/Array and why they are difficult + the random has its limits
% \begin{itemize}
%    \item autonomy
%    \item low inference delay leads to unsuccessful type inferences
%    \item sometimes certain coverage simply \emph{cannot} be achieved with a certain length
%    \item however the effect of length is highly variable, it speeds up but after at some point it slows down the test gen process --- hence why a search-based technique should be used as the random is not optimal (the optimal length is highly variable)
%    \item random still achieves good results but it is helped by the custom data generators and the pooling system + our tests lend themselves well to that
%    \item threats to validity: size of programs... + the fact that we don't deal with array or function as they are key js objects. discuss why we don't deal with them
%    \item nonetheless based on what we set out to do, the work can be considered successful as full coverage is achieved for almost all methods in under 20 seconds, with a variety of different types and scenarios
% \end{itemize}
